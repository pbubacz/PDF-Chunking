{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supporting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "def print_chunks_page_content(page_content,sparse=False):\n",
    "    print(f\"Number of chunks: {len(page_content)}\")\n",
    "    for i, chunk in enumerate(page_content):\n",
    "        print(f\"Chunk {i + 1} character count: {len(chunk.page_content)} token number: {num_tokens_from_string(chunk.page_content)}\" )\n",
    "        if not sparse:\n",
    "            print(chunk.page_content)        \n",
    "        else:\n",
    "            print(chunk.page_content [:50])\n",
    "        print(\"Meta data: \", chunk.metadata)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text using OpenSource solution - PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = (\"sample.pdf\")\n",
    "loader = PyPDFLoader(file_path)\n",
    "pdf_pages = loader.load_and_split()\n",
    "print_chunks_page_content(pdf_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_chunks_page_content(pdf_pages,sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "rct_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "splits = rct_text_splitter.split_documents(pdf_pages)\n",
    "\n",
    "print_chunks_page_content(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_chunks_page_content(splits, sparse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text using Document Intelligence API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "file_path = (\"sample.pdf\")\n",
    "\n",
    "doc_intelligence_endpoint = os.getenv(\"DOCUMENTINTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"DOCUMENTINTELLIGENCE_API_KEY\")\n",
    "\n",
    "loader = AzureAIDocumentIntelligenceLoader(\n",
    "    api_endpoint=doc_intelligence_endpoint, \n",
    "    api_key=doc_intelligence_key, \n",
    "    file_path=file_path, \n",
    "    api_model=\"prebuilt-layout\", \n",
    "    mode=\"markdown\"\n",
    ")\n",
    "di_documents = loader.load()\n",
    "with open(\"md_sample.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(di_documents[0].page_content)\n",
    "print_chunks_page_content(di_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\"),  \n",
    "    (\"#######\", \"Header 7\"), \n",
    "    (\"########\", \"Header 8\")\n",
    "]\n",
    "md_text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "md_header_splits = md_text_splitter.split_text(di_documents[0].page_content)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(md_header_splits)))\n",
    "print_chunks_page_content(md_header_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_chunks_page_content(md_header_splits, sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "rct_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "splits = rct_text_splitter.split_documents(md_header_splits)\n",
    "\n",
    "print_chunks_page_content(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_chunks_page_content(splits, sparse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "NEW_AFTER_N_CHARS = 1000\n",
    "MAX_CHARACTERS = 1000\n",
    "COMBINE_UNDER_N_CHARS = 300\n",
    "\n",
    "elements = partition_md(text=di_documents[0].page_content)\n",
    "print (f\"Number of elements: {len(elements)}\")\n",
    "\n",
    "chunks = chunk_by_title(elements, multipage_sections=True, max_characters=MAX_CHARACTERS, new_after_n_chars=NEW_AFTER_N_CHARS, combine_text_under_n_chars=COMBINE_UNDER_N_CHARS)  \n",
    "out_text=''\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):           \n",
    "    if chunk.category == 'Table':\n",
    "        chunk_text = chunk.metadata.text_as_html\n",
    "    else:\n",
    "        chunk_text = chunk.text\n",
    "    print(f'Chunk {i} ({chunk.category}): Chunk len ({len(chunk_text)}) Chunk token ({num_tokens_from_string(chunk_text)}) \\n{chunk_text}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import semchunk\n",
    "chunker = semchunk.chunkerify('gpt-4', chunk_size=250) \n",
    "data= chunker(di_documents[0].page_content) \n",
    "\n",
    "def print_chunks(list_chunks):\n",
    "    print(f\"Number of chunks: {len(list_chunks)}\")\n",
    "    for i, chunk in enumerate(list_chunks):\n",
    "        print(f\"\\nChunk {i + 1} character count: {len(chunk)} token number: {num_tokens_from_string(chunk)}\" )\n",
    "        print(chunk)\n",
    "print_chunks (data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
